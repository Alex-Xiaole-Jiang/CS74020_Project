{"cells":[{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["import numpy as np\n","import torch\n","from sentence_transformers import SentenceTransformer, InputExample, losses, models\n","from torch.utils.data import DataLoader\n","from torch import nn\n","from torch.nn import functional as F\n","\n","from datasets import load_dataset, Dataset\n","\n","from torchinfo import summary"]},{"cell_type":"markdown","metadata":{},"source":["### My pooling method"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":["class learnable_gaussian(nn.Module):\n","    '''\n","    I can weight something by a gaussian and let the network learn how wide the gaussian should be\n","    '''\n","    def __init__(self, center, var = 128):\n","        super().__init__()\n","        self.var = nn.Parameter(data =  torch.Tensor([var]))\n","        self.center = center\n","    def forward(self, x):\n","        return torch.exp(-(x - self.center)**2/(2 * self.var))\n","\n","class SwiGLU(nn.Module):\n","    '''\n","    Basically SwiGLU from ChatGPT, looks correct\n","    '''\n","    def __init__(self, input_dim, hidden_dim, output_dim, use_layernorm = False):\n","        super().__init__()\n","        self.fc1 = nn.Linear(input_dim, hidden_dim)\n","        self.fc2 = nn.Linear(input_dim, hidden_dim)\n","        self.output_layer = nn.Linear(hidden_dim, output_dim)\n","        self.use_layernorm = use_layernorm\n","        if self.use_layernorm:\n","            self.ln = nn.LayerNorm(hidden_dim)\n","\n","    def forward(self, x):\n","        x1 = self.fc1(x)\n","        x2 = self.fc2(x)\n","        output = self.output_layer(F.silu(x1) * x2)\n","        if self.use_layernorm:\n","            return self.ln(output)\n","        return output\n","\n","class SwiGLUNetwork(nn.Module):\n","    '''\n","    Another piece of GPT code with minor modifications\n","    Since we just want a one sentence embedding I am just setting output_dim = 1\n","    '''\n","    def __init__(self, input_dim, mid_dim = 64, use_layernorm = True):\n","        super(SwiGLUNetwork, self).__init__()\n","        self.layers = nn.ModuleList()\n","        depth_dim = np.arange(start = 0, stop = input_dim+1, step = 128)\n","        depth_dim[0], depth_dim[-1] = mid_dim, input_dim\n","        depth_dim = depth_dim[::-1]\n","\n","        # Add subsequent layers\n","        for i in range(len(depth_dim) - 1):\n","            self.layers.append(SwiGLU(depth_dim[i], depth_dim[i], depth_dim[i+1]))\n","        self.layers.append(SwiGLU(mid_dim, mid_dim, 1, use_layernorm = False))\n","\n","    def forward(self, x):\n","        for layer in self.layers:\n","            x = layer(x)\n","        return x\n","\n","\n","class my_pooling(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.ffn_512 = SwiGLUNetwork(512)\n","        self.ffn_384 = SwiGLUNetwork(384)\n","        self.ffn_256 = SwiGLUNetwork(256)\n","        self.ffn_128 = SwiGLUNetwork(128)\n","        \n","        self.gaussian_512 = learnable_gaussian(512)\n","        self.gaussian_384 = learnable_gaussian(384)\n","        self.gaussian_256 = learnable_gaussian(256)\n","        self.gaussian_128 = learnable_gaussian(128)\n","\n","        self.unconventional_pooling_weight = nn.Parameter(data =  torch.Tensor([0.01]))\n","    \n","    @staticmethod\n","    def linear_interpolate_sentence(masked_embeddings_tranposed, embed_length):\n","        embedding_512 = torch.empty(0)\n","        embedding_384 = torch.empty(0)\n","        embedding_256 = torch.empty(0)\n","        embedding_128 = torch.empty(0)\n","        \n","        for sentence, length in zip(masked_embeddings_tranposed, embed_length):\n","            sentence_512 = F.interpolate(sentence[:, 0:length].unsqueeze(0), size = 512, mode = 'linear')\n","            sentence_384 = F.interpolate(sentence[:, 0:length].unsqueeze(0), size = 384, mode = 'linear')\n","            sentence_256 = F.interpolate(sentence[:, 0:length].unsqueeze(0), size = 256, mode = 'linear')\n","            sentence_128 = F.interpolate(sentence[:, 0:length].unsqueeze(0), size = 128, mode = 'linear')\n","\n","            embedding_512 = torch.cat((embedding_512, sentence_512), dim = 0)\n","            embedding_384 = torch.cat((embedding_384, sentence_384), dim = 0)\n","            embedding_256 = torch.cat((embedding_256, sentence_256), dim = 0)\n","            embedding_128 = torch.cat((embedding_128, sentence_128), dim = 0)\n","        \n","        return embedding_512, embedding_384, embedding_256, embedding_128\n","\n","\n","    def forward(self, features):\n","        # every sentence get's a mask that masks the rest of the length\n","        token_embeddings = features[\"token_embeddings\"]\n","        attention_mask = features[\"attention_mask\"]\n","        embed_length = attention_mask.sum(1) # number from 0 to 512 which is embed length\n","        input_mask_expanded = (\n","            attention_mask.unsqueeze(-1).expand(token_embeddings.size()).to(token_embeddings.dtype)\n","            )\n","        masked_embeddings = token_embeddings * input_mask_expanded # this is the real sentence\n","        mean_embeddings = torch.sum(masked_embeddings, 1)/torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n","        mean_embeddings = F.normalize(mean_embeddings, p=2, dim=1)\n","        fnn_raw_embeddings = torch.transpose(token_embeddings, -2, -1) # put 512 in last dim for linear layer\n","        fnn_512_embeddings, fnn_384_embeddings, fnn_256_embeddings, fnn_128_embeddings = self.linear_interpolate_sentence(fnn_raw_embeddings, embed_length)\n","        \n","        sum_unconv_embed = (\n","            self.ffn_512(fnn_512_embeddings) * self.gaussian_512(embed_length)[:, None, None] +\n","            self.ffn_384(fnn_384_embeddings) * self.gaussian_384(embed_length)[:, None, None] +\n","            self.ffn_256(fnn_256_embeddings) * self.gaussian_256(embed_length)[:, None, None] + \n","            self.ffn_128(fnn_128_embeddings) * self.gaussian_128(embed_length)[:, None, None])\n","\n","        output_mod = (sum_unconv_embed * self.unconventional_pooling_weight).squeeze() \n","        output = F.normalize(mean_embeddings + output_mod, p=2, dim=1)\n","\n","        features.update({\"sentence_embedding\": output})\n","        return features"]},{"cell_type":"markdown","metadata":{},"source":["### Let's test and see that it runs"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["sentence_trans = SentenceTransformer('sentence-transformers/multi-qa-distilbert-cos-v1')\n","sentence_trans.eval();\n","# model._first_module()\n","# sentence_trans"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["unconv_pooling = my_pooling()\n","my_pooling_model = SentenceTransformer(modules=[sentence_trans[0], unconv_pooling])\n","my_pooling_model.eval();"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["dataset = load_dataset(\"mteb/cqadupstack-physics\", \"corpus\")"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[{"data":{"text/plain":["[\"Let's discuss about $SU(3)$. I understand that the most important representations (relevant to physics) are the defining and the adjoint. In the defining representation of $SU(3)$; namely $\\\\mathbf{3}$, the Gell-Mann matrices are used to represent the generators $$ \\\\left[T^{A}\\\\right]_{ij} = \\\\dfrac{1}{2}\\\\lambda^{A}, $$ where $T^A$ are the generators and $\\\\lambda^A$ the Gell-Mann matrices. In adjoint representation, on the other hand, an $\\\\mathbf{8}$, the generators are represented by matrices according to $$ \\\\left[ T_{i} \\\\right]_{jk} = -if_{ijk}, $$ where $f_{ijk}$ are the structure constants. My question is this, how can one represent the generators in the $\\\\mathbf{10}$ of $SU(3)$, which corresponds to a symmetric tensor with 3 upper or lower indices (or for that matter how to represent the $\\\\mathbf{6}$ with two symmetric indices). What is the general procedure to represent the generators in an arbitrary representation?\",\n"," 'So in the context of a set of notes I am reading about acoustics I get to equation (23) in this paper. Basically it comes down to showing that ( **note the dots above the a\\'s meaning time derivative!)** $$ f(t) = -a_0c_0\\\\int_{-\\\\infty}^t \\\\dot{a}(t\\'+\\\\frac{a_0}{c_0})e^{-\\\\frac{c_0}{a_0}(t-t\\')} dt\\'\\\\ = -a_0^2\\\\dot{a}(t),. $$ under the assumption that $\\\\frac{c_oT}{a_0} >>1$. $T$ can be thought of as a characteristic time scale for this problem. For those interested in more than just the math, please see the paper. Now I show this in the following way: Integrating by parts: $$ \\\\begin{align} f(t) = & \\\\\\\\\\\\ =& -a_0^2\\\\dot{a}(t+\\\\frac{a_0}{c_0})+a_0c_0\\\\int_{-\\\\infty}^t \\\\ddot{a}(t\\'+\\\\frac{a_0}{c_0})\\\\frac{a_0}{c_0}e^{-\\\\frac{c_0}{a_0}(t-t\\')} dt\\'\\\\\\\\\\\\\\\\\\\\ & \\\\end{align} $$ By expanding $\\\\dot{a}$ as a taylor series we get: $$ \\\\dot{a}(t+\\\\frac{a_0}{c_0}) = \\\\dot{a}(t) +\\\\frac{a_0}{c_0}\\\\ddot{a}(t)+O\\\\left(\\\\frac{a_0^2}{c_0^2}\\\\right) $$ Now if we make the order of magnitude estimate: $$ \\\\frac{a_0}{c_0}\\\\ddot{a}(t) \\\\simeq \\\\frac{a_0}{c_0T}\\\\dot{a}(t) $$ then I hope it is clear by applying this reasoning to the equation for $f(t)$ that we have shown $f(t) = -a_0^2\\\\dot{a}(t)$ for $\\\\frac{c_oT}{a_0} >>1$. **Now here comes my question:** How can I justify the time derivative operator behaving as $1/T$, where $T$ was just given as the length scale of the problem (which seems so arbitrary)? Is my reasoning above correct? I was hoping someone could put my mind at ease about the \"hand wavy\" nature of these order of magnitude approximations.',\n"," \"I need to shield my device from magnetic interference, including earth magnetic field (if you move device around, it might be enough to cause slight currents i guess) and magnetic field caused by power nets, wires with large currents e.t.c . I know this could be achieved by making case out of mu-metal, but it seems that I can't find it anywhere (especially in small quantities). So, are the other, easier ways to shield magnetic field? Will multiple sheets of steel work for example?\",\n"," 'The actuator of a hard drive head consists of two very strong neodymium magnets, with an electromagnetic coil between them. If you take that apart, the magnets attract each other very strongly. There\\'s no doubt the field between them is very strong. But if you flip them back to back, there is no repulsion force - there is pretty much nothing. While the magnets are very strong on one side, they seem completely inert on the other. I understand how a U-shape of a shaped magnet allows it to focus field near the two exposed poles, but how does a flat rectangular plate manage to keep the field over one flat, wide surface and nearly none near the other? [sorry about the heavy edit but it seems the question got totally muddled with irrelevant discussion about unipolar magnets and possibility or impossibility to remove magnetic field from one side. Only heavy rephrasing may help.] Edit: some photos: 1: The magnets stuck together. They hold really hard, I\\'d be unable to just pull them apart but I can slide one against the other to separate them. ![1](http://i.stack.imgur.com/pTjpC.jpg) The magnets in \"inert position\" - they don\\'t act on each other at all, just like two inert pieces of metal. ![2](http://i.stack.imgur.com/0HTae.jpg) The magnets seem to have two poles located on the surface off-center. Here, a normal magnet placed against the hard disk magnet, centering itself on one side, then flipped - on another. ![3](http://i.stack.imgur.com/AvckM.jpg) ![4](http://i.stack.imgur.com/nG9EA.jpg) ![5](http://i.stack.imgur.com/UBtIH.jpg) The metal shield seems to act like completely unmagnetized ferromagnetic. I can stick the \"normal magnet\" any way I like to it, and it doesn\\'t act on another piece of ferromagnetic (a needle) at all. ![6](http://i.stack.imgur.com/9iqcP.jpg)![7](http://i.stack.imgur.com/Mn1EC.jpg) When I apply a small magnet to it, it becomes magnetic like any normal \"soft\" ferromagnetic - attracts the needle weakly. It behaves as if the (very powerful) neodymium magnet glued to the other side wasn\\'t there at all. ![8](http://i.stack.imgur.com/yrW8S.jpg) Unfortunately the neodymium magnets are glued very well and so fragile I was unable to separate any without snapping it, and then the \"special properties\" seem to be gone.',\n"," 'IANAP, so feel free to berate me for thinking apocryphal thoughts! Just as magnetism has two charges, in which particles of like-charge repulse and particles of dissimilar charge attract, might gravity have two charges in which particles of like-charge attract and particles of dissimilar charge repulse? In practice, the state of magnetism means that there is no system composed of many particles in which all particles attract. Rather, there is a net 0 charge if there are equal numbers of each particle type. My silly theory regarding gravity would mean in practice that there would be two (or more) \"clumps\" (or universes) in existence, which are racing away from each other. So in our clump (universe) we see only attracting particles, because all the opposing particles have long since separated out and are racing away beyond the boundary of the observable universe. Just like the alien who lands in China and assumes that all humans have slanted eyes, we only observe the attracting particles (or \"charge\") and disregard the other, unobservable, \"charge\". Is there any way to disprove this idea, or like string theory can I go one believing it as it can never be disproved? Thanks.']"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["example_sentence = dataset['corpus']['text'][0:5]\n","example_sentence"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["with torch.no_grad():    \n","    tokenized = sentence_trans[0].tokenize(example_sentence)\n","    my_tokenized = my_pooling_model[0].tokenize(example_sentence)\n","    built_in_out = sentence_trans(tokenized)\n","    my_out = my_pooling_model(my_tokenized)"]},{"cell_type":"markdown","metadata":{},"source":["Ignore above, use below for encode"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[],"source":["with torch.no_grad():\n","    built_in_embedding = sentence_trans.encode(example_sentence)\n","    my_embedding = my_pooling_model.encode(example_sentence)\n"]},{"cell_type":"markdown","metadata":{},"source":["we see that it is almost the same, indeed, it starts out as a small modification, but I assume we can train and the modifcation will be large"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"data":{"text/plain":["array([[9.9465251e-06, 9.9614263e-06, 9.9707395e-06, ..., 9.8515302e-06,\n","        9.8710880e-06, 9.8878518e-06],\n","       [2.4335459e-05, 2.4091452e-05, 2.3666769e-05, ..., 2.3588538e-05,\n","        2.4154317e-05, 2.4106121e-05],\n","       [1.6091019e-04, 1.6538799e-04, 1.6475096e-04, ..., 1.6450509e-04,\n","        1.6423408e-04, 1.6508624e-04],\n","       [2.4216250e-05, 2.4129637e-05, 2.3532659e-05, ..., 2.4538487e-05,\n","        2.3966655e-05, 2.4194364e-05],\n","       [5.8986247e-05, 5.8498234e-05, 5.8483332e-05, ..., 5.8760867e-05,\n","        5.8815815e-05, 5.8639795e-05]], dtype=float32)"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["built_in_embedding - my_embedding"]},{"cell_type":"markdown","metadata":{},"source":["my pooling roughly adds 2 million parameters"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"data":{"text/plain":["=====================================================================================\n","Layer (type:depth-idx)                                       Param #\n","=====================================================================================\n","SentenceTransformer                                          --\n","├─Transformer: 1-1                                           --\n","│    └─DistilBertModel: 2-1                                  --\n","│    │    └─Embeddings: 3-1                                  23,835,648\n","│    │    └─Transformer: 3-2                                 42,527,232\n","├─my_pooling: 1-2                                            1\n","│    └─SwiGLUNetwork: 2-2                                    --\n","│    │    └─ModuleList: 3-3                                  1,330,689\n","│    └─SwiGLUNetwork: 2-3                                    --\n","│    │    └─ModuleList: 3-4                                  608,385\n","│    └─SwiGLUNetwork: 2-4                                    --\n","│    │    └─ModuleList: 3-5                                  214,145\n","│    └─SwiGLUNetwork: 2-5                                    --\n","│    │    └─ModuleList: 3-6                                  49,665\n","│    └─learnable_gaussian: 2-6                               1\n","│    └─learnable_gaussian: 2-7                               1\n","│    └─learnable_gaussian: 2-8                               1\n","│    └─learnable_gaussian: 2-9                               1\n","=====================================================================================\n","Total params: 68,565,769\n","Trainable params: 68,565,769\n","Non-trainable params: 0\n","====================================================================================="]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["summary(my_pooling_model)"]}],"metadata":{"kernelspec":{"display_name":"sentence-embedding","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":2}
