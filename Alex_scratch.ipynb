{"cells":[{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Alex\\.conda\\envs\\sentence-embedding\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Iteration: 100%|██████████| 1/1 [00:00<00:00,  1.28it/s]\n","Epoch: 100%|██████████| 1/1 [00:00<00:00,  1.27it/s]\n"]}],"source":["from sentence_transformers import SentenceTransformer, InputExample, losses\n","from torch.utils.data import DataLoader\n","\n","# Define the model. Either from scratch of by loading a pre-trained model\n","model = SentenceTransformer(\"distilbert-base-nli-mean-tokens\")\n","\n","# Define your train examples. You need more than just two examples...\n","train_examples = [\n","    InputExample(texts=[\"My first sentence\", \"My second sentence\"], label=0.8),\n","    InputExample(texts=[\"Another pair\", \"Unrelated sentence\"], label=0.3),\n","]\n","\n","# Define your train dataset, the dataloader and the train loss\n","train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n","train_loss = losses.CosineSimilarityLoss(model)\n","\n","# Tune the model\n","model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100)"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Alex\\.conda\\envs\\sentence-embedding\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from sentence_transformers.cross_encoder import CrossEncoder\n","model = CrossEncoder(\"distilroberta-base\", num_labels = 1)"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[{"data":{"text/plain":["RobertaForSequenceClassification(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0-5): 6 x RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): RobertaClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=1, bias=True)\n","  )\n",")"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["model.model"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Alex\\.conda\\envs\\sentence-embedding\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"text/plain":["Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: DistilBertModel "]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["model = SentenceTransformer(\"distilbert-base-nli-mean-tokens\")\n","model._first_module()"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"data":{"text/plain":["DistilBertModel(\n","  (embeddings): Embeddings(\n","    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","    (position_embeddings): Embedding(512, 768)\n","    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (transformer): Transformer(\n","    (layer): ModuleList(\n","      (0-5): 6 x TransformerBlock(\n","        (attention): MultiHeadSelfAttention(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (ffn): FFN(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          (activation): GELUActivation()\n","        )\n","        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","    )\n","  )\n",")"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["(model[0].auto_model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"sentence-embedding","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":2}
